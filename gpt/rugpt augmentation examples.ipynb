{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'karelian'\n",
    "real_name = 'Karjalan'\n",
    "gpt_name = 'figpt' #rugpt\n",
    "translation_word = 'Käännös' # Перевод\n",
    "#Karjalan Lyydin\n",
    "# вепсский людиковский карельский селькупский кетский чукотский эвенкийский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anmosolova/.cache/huggingface/transformers\n"
     ]
    }
   ],
   "source": [
    "from transformers import file_utils\n",
    "print(file_utils.default_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "tok = GPT2Tokenizer.from_pretrained(\"finnish_models/weights\") #загрузка дообученного токенизатора \n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"finnish_models/weights\") #загрузка дообученной модели\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of parallel sentences generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    inpt = tok.encode(\"Карельский:\", return_tensors=\"pt\")\n",
    "    out = model.generate(inpt.cuda(),\n",
    "                                       num_return_sequences= 2,\n",
    "                                       max_length=100, \n",
    "                                       repetition_penalty=2.0,\n",
    "                                       do_sample=True,\n",
    "                                       top_k=50, top_p=0.95,\n",
    "                                       temperature=0.7)\n",
    "    print(tok.decode([x.item() if x != 50257 else 0 for x in out[0]]).split('<pad>')[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyydin: vot siksei kirguttih zubari. Käännös: ар�е�<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "----\n",
      "Lyydin: oli naišton da siit küläd opädäh sinnä. Käännös быĂ <|endoftext|>\n",
      "----\n",
      "Lyydin: nu otamme nügö ned muštan dai mad�уuskuoin kõlä tämä on. Käännös: в����<|endoftext|>\n",
      "----\n",
      "Lyydin: siid oli gordin dakun rodut sigä kävelttih därvez. Käännös: то��ă��е<|endoftext|>\n",
      "----\n",
      "Lyydin: nu siid hüö elettih. Käännös: от<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "----\n",
      "Lyydin: a muid ni kedä olnu ei. Käännös: пр<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "----\n",
      "Lyydin: nu siid dälgele do levoskan guban taga šuojun doged müöti lähtedih ka enämbi ni olnu ei. Käännös sanalle suuntanumero Ilmainen Lue tämä seloste huolellisesti, ennen kuin aloitat lääkkeen ottamisen tai lopiihezevan da dengat se kondruoin poige toine oli männú hôvä leski pani pielnun pečuriin elaig\n",
      "----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m      2\u001b[0m     inpt \u001b[38;5;241m=\u001b[39m tok\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLyydin:\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tok\u001b[38;5;241m.\u001b[39mdecode([x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m50257\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m]])\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/mtenv/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mtenv/lib/python3.9/site-packages/transformers/generation_utils.py:1320\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1313\u001b[0m         input_ids,\n\u001b[1;32m   1314\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mnum_return_sequences,\n\u001b[1;32m   1315\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1316\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1317\u001b[0m     )\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;66;03m# 12. run sample\u001b[39;00m\n\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_return_sequences \u001b[38;5;241m>\u001b[39m num_beams:\n",
      "File \u001b[0;32m~/mtenv/lib/python3.9/site-packages/transformers/generation_utils.py:1938\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1935\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 1938\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   1946\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/mtenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mtenv/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1048\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1048\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/mtenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mtenv/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:891\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    881\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    882\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    883\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    888\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 891\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/mtenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mtenv/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:391\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    389\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    390\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 391\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    400\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/mtenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mtenv/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:321\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_past \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    320\u001b[0m     past_key, past_value \u001b[38;5;241m=\u001b[39m layer_past\n\u001b[0;32m--> 321\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpast_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((past_value, value), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    inpt = tok.encode(\"Lyydin:\", return_tensors=\"pt\")\n",
    "    out = model.generate(inpt.cuda(),\n",
    "                                       num_return_sequences= 2,\n",
    "                                       max_length=100, \n",
    "                                       repetition_penalty=2.0,\n",
    "                                       do_sample=True,\n",
    "                                       top_k=50, top_p=0.95,\n",
    "                                       temperature=0.7)\n",
    "    print(tok.decode([x.item() if x != 50257 else 0 for x in out[0]]).split('<pad>')[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эвенкийский: такти кэхэдэм аракукан. Перевод: стак мучаюсь помаленьку \n",
      "----\n",
      "Эвенкийский: аӈил орор һавалдяфкāл. Перевод: эти оленей держали чтобы они работали на звероферме или ферме были бы если б не это обрезание веток осенью весной изгородь делали так же как и летом подстригают кусты все деревья зимой выравнивают делают каркас для чума ставят потом опять убирают снег весь этот самый расчищают площадку снова вывозят грязь с этой площадки уезжают всё лето эта площадка чистота такая\n",
      "----\n",
      "Эвенкийский: бэел дикончэлдеми. Перевод: люди прячутся \n",
      "----\n",
      "Эвенкийский: аӈя окэл гунэн. Перевод: это самое сделай она сказала \n",
      "----\n",
      "Эвенкийский: и всё. Перевод: да \n",
      "----\n",
      "Эвенкийский: ē hувулитын. Перевод: да всевсе их там полно народуто кэтэ бисо̄тин ну и т п д е аргишили наверное тоже не хватало людей то есть волков совсем уж точно а раньше всегда было много народа тогда когда волки были очень даже почетные гости если сказать прямо так это олени бывали или как говорят сейчас любят говорить волкодайи эти самые ага короче говоря люди которые сопровождали того человека идущего навстречу им\n",
      "----\n",
      "Эвенкийский: этэрэв гада. Перевод: не возьмем \n",
      "----\n",
      "Эвенкийский: ама̄канин варак только нуӈан һа̅кчадяран. Перевод: дедушка враг пинает очень сильно болит живот \n",
      "----\n",
      "Эвенкийский: ну в берлогето это. Перевод: да конечно там \n",
      "----\n",
      "Эвенкийский: нуӈан билдэн. Перевод: он остался \n",
      "----\n",
      "Эвенкийский: hулан. Перевод: лиса \n",
      "----\n",
      "Эвенкийский: тадук hэлэ эпет дюла̄и ирэн комоеми. Перевод: потом вот опять в дом зашел Комое \n",
      "----\n",
      "Эвенкийский: бикэ чо. Перевод: ято что \n",
      "----\n",
      "Эвенкийский: ну что еще. Перевод: да все \n",
      "----\n",
      "Эвенкийский: дюр hутэн тадӯ бидерэ младшиелви. Перевод двое детей там \n",
      "----\n",
      "Эвенкийский: илкэ̄ндуло́тынтэл. Перевод: иди по их зарубкам \n",
      "----\n",
      "Эвенкийский: тар аhӣ hимула̄дявкā. Перевод: та женщина молчит \n",
      "----\n",
      "Эвенкийский: а для них. Перевод: нет \n",
      "----\n",
      "Эвенкийский: нуӈан гунэн. Перевод: он сказал \n",
      "----\n",
      "Эвенкийский: кэлэ. Перевод: ну \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    inpt = tok.encode(\"Эвенкийский:\", return_tensors=\"pt\")\n",
    "    out = model.generate(inpt.cuda(),\n",
    "                                       num_return_sequences= 2,\n",
    "                                       max_length=100, \n",
    "                                       repetition_penalty=2.0,\n",
    "                                       do_sample=True,\n",
    "                                       top_k=50, top_p=0.95,\n",
    "                                       temperature=0.7)\n",
    "    print(tok.decode([x.item() if x != 50257 else 0 for x in out[0]]).split('<pad>')[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Селькупский: tɛ mumpa kučale qondäeŋ. Перевод: вы мол никуда не уходите пока я болею ваш дом стоит без дверей и половицы в кровище снаружи его жилище внутри него спит мёртвым сном голодная собака на улице воет от голодавший человек у себя дома должен спать одетым по-зимнему одежда снята кожа почернела подмышками снег таять начал когда тепло стало промозглый вечерний воздух\n",
      "----\n",
      "Селькупский: nɨnä čarrɛsa. Перевод: потом он почернел от сажи и дыма костра стал белый как снег ночи без звездное небо ночь такая глубокая ночная тьма в которой едва светится еле видная полоска света уходящего за горизонт к белому не родному солнцу земли его родины на той стороне горизонта там где земля кончается называется белая степь с черными песками озерами болотистыми реками заросшими камышом берегами со\n",
      "----\n",
      "Селькупский: nɨnä təp pona qaj moškat ɔmtak. Перевод: потом он брат в парке из мешковины на улицу вышел немного пройдя снова упал без силкнувшись с кемто вроде бы тоже вышедшим наружу человеком по имени Павелконино имя было у него когданибудь спросил его отец вотвот должен был придти домой и забрать свое чадо спать вместе со своим малышом но не пришел а лежал там тихо\n",
      "----\n",
      "Селькупский: ijatɨ nıl kətättoqon. Перевод: сын так сказал \n",
      "----\n",
      "Селькупский: qumɨt imannɔl ponä mannaj mačontane ütt. Перевод: люди сватаются друг с другом в лесу на берегу моря из мешковины чёрт выходит к людям обращается сказал он потом немного погодя опять появился будто так и было всегда вот только человек показался что ли вышел вроде как по снегу шёл снег таять начал а сам всё ещё снегом хрустит слегка покачиваясь стоит подёргав головой чтобы хоть\n",
      "----\n",
      "Селькупский: täp nennamɔssa imlantɨkinı kinop soldatka. Перевод: он рассердился и сказал бабушке \n",
      "----\n",
      "Селькупский: ukkur tät moqonɨ. Перевод; однажды он домой приехал на лошади без узды и с больной шеей в больнице лежал долго ли коротко время прошло потом его отпустили вот только голова немного кружилась так сильно качалась из стороны сторонувниз покачиваясь как будто бы говоря что у него силы нет совсем обессилел нука где моя лошадь туда пусть она идет пешком по дороге сказал царь гор ночевал опять дома остался ночевать придя утром поехал работать\n",
      "----\n",
      "Селькупский: pɨrni karra čap tüŋak muqultiril pol matɛlä noptiti. Перевод: ведьма едва подошла к очагу как в огонь упала сама крича от боли и испугавшись этого звука эхом раздавшегося среди бревенчатого домаочага охнула ойкая поднялась на ноги отряхивая снег с одежды догоняя убежавшую было прочь женщину сказала что я тут немного посижу пока болею она\n",
      "----\n",
      "Селькупский: ukkɨr ijajä soqošpɔtet. Перевод: один мальчишка в воду нырнул за товаром для рыбкопа его товарищи по воде нырять будут искать \n",
      "----\n",
      "Селькупский: täp koptaqɨntɔtet. Перевод: они лёгкие заболели \n",
      "----\n",
      "Селькупский: nɨnä təp pona ite imat. Перевод: потом он брат в парке из мешковины на улицу вышел \n",
      "----\n",
      "Селькупский: tat konna qəlla apsol amtɨ. Перевод: ты иди на берег поешь еды чтобы потом пойти спать в кровать к своей девушке \n",
      "----\n",
      "Селькупский: ijakɔtɨl imat ɛppant. Перевод: бездетная жена у него \n",
      "----\n",
      "Селькупский: qumɨt kosti mɔttä tultila pon. Перевод : люди в гости придут а вещи все на улице лежат безхозные штаны его одежда вся изорвананая обувь обходит вокруг него кругом смотрит что бы ктонибудь не наступил ненцы ведь он тоже босиком ходит по снегу снаружи дома сидят голодно спят одетые кожа почернела от морды их лошади еле дышавшие бокари снег валят наружу когда заходи солнце садится спать\n",
      "----\n",
      "Селькупский: täpɨt qaip kočeɣondot a nut. Перевод: они не знали что это чертёнок сидит голодный \n",
      "----\n",
      "Селькупский: na tuntɨsä kuttar ɛŋant. Перевод: как ты сказал \n",
      "----\n",
      "Селькупский: nälqup mannɨmpatij puŋa. Перевод; девушка видит что её парня бьют \n",
      "----\n",
      "Селькупский: nɨnä moqona tüla aj ılla olɔtat üttet. Перевод: потом он домой приехал и опять спать лёг заснул \n",
      "----\n",
      "Селькупский: ičakiatɨ ətäkɔl. Перевод: ичакичика молча сидит \n",
      "----\n",
      "Селькупский: čunta sarältɨ moqona tüŋa. Перевод : лошадь запрягай чтобы отвезти молоко домой на пастбище стояло оно изза горизонта светлое пятно \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    inpt = tok.encode(\"Селькупский:\", return_tensors=\"pt\")\n",
    "    out = model.generate(inpt.cuda(),\n",
    "                                       num_return_sequences= 2,\n",
    "                                       max_length=100, \n",
    "                                       repetition_penalty=2.0,\n",
    "                                       do_sample=True,\n",
    "                                       top_k=50, top_p=0.95,\n",
    "                                       temperature=0.7)\n",
    "    print(tok.decode([x.item() if x != 50257 else 0 for x in out[0]]).split('<pad>')[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кетский: улесь датпияқ. Перевод: дождь полил \n",
      "----\n",
      "Кетский: бу дананберольбет. Перевод : пекла хлеб \n",
      "----\n",
      "Кетский: аб қип ат дахун эсяуӷон. Перевод на озеро я потом пришла к самой воде \n",
      "----\n",
      "Кетский: қиб это. Перевод : дедэто\n",
      "Белоруссия перевод неясен люди русские плохо знают географию говорит им учительница географии сказала что в низовье живет рыбачит обычно берет рыбу чтобы людям не дать ее съесть потомучто я сама видела ведь много людей ели рыбину после этого мне еще лет десять было наверное ну и ладно тогда уже взрослые дети были мои братья да сестрам тоже по пять или шесть братьев у меня самой то есть рыбы всегда была очень немного бывало она\n",
      "----\n",
      "Кетский: дең талын тан дигибетин. Перевод: люди муку собирали \n",
      "----\n",
      "Кетский: я сытый целый день. Перевод и перевод неясны \n",
      "----\n",
      "Кетский: это конечно давно уже было в семьдесят четвертом году. Перевод (это наверное совсем уж была большая пенсия \n",
      "----\n",
      "Кетский: ат қая бән дигдоолбет. Перевод : я потом о них заботилась сама была ребенком очень сильно болела мать умерла отец умер весной здесь в деревне весенние чумища приходят к самой мадуйке люди рыбачат обычно у нее зимой тоже убивают рыбу чтобы не мерзла она летом живет на реке большая вода снег же проваливается глубокий до середины бедер человек говорит мне это снится целый месяц сниться начинает людям почемуто ведь март самый длинный зимний\n",
      "----\n",
      "Кетский: қаря буд хыб хай. Перевод и перевод неясны есть еще ктонибудь кроме меня самой бабушка ведь уже давно умерла я осталась совсем одна мне лет десять было летом этого года здесь в деревне отец с матерью умерли это очень далеко отсюда вот так что никого не обижали ни одного человека хорошо помню только бабушку одну хорошую была женщина хоть какуюто жизнь людям испортили она болела сильно тяжело вставать заставляла ходила потом тоже заболела тяжелой болезнью умер папа а мать почему то раньше времени\n",
      "----\n",
      "Кетский: бу си дугде бат дильден. Перевод: он ночь целую плакал \n",
      "----\n",
      "Кетский: там и колокол большой. Перевод : както я уже забыла теперь \n",
      "----\n",
      "Кетский: а сёоң и ын десьтэйта. Перевод : на рыбалке дрова заготавливаем в день тринадцатое число месяца весеннего равноденствия / март самый/ апрель самая большая вода наступает обычно у людей рыбачитов месяц апреле самые сильные люди работают апреля я сытый целый год это мне четыре года было весной когда весна наступила очень сильно людям плохо стало April двадцать первое декабря много зверей зимовать стали December ночь длинная была March пятьдесятого марта\n",
      "----\n",
      "Кетский: қибада баам хай сюуугди кәйган биснемин. Перевод перевод деда шаманскую парку прочь возьмите меня унесите домой в деревню дедом клянусь вам это было двадцать лет назад я сейчас здесь живу не могу идти больше скажу отцу пусть он хоть выносит мне парки из огня ведь они горят очень сильно у отца парка большая сила духа теперь осталась только одна бабушка и еще один дедушка ну ладно духи пламени вниз по реке бегут чтобы\n",
      "----\n",
      "Кетский: тона. Перевод неясен \n",
      "----\n",
      "Кетский: бу динбесь наңа. Перевод: он подошел к ним \n",
      "----\n",
      "Кетский: қя хыб средняя аб хунась школади кәйкет огон. Перевод (дольше всех в школе я училась) Хата эсонда илля барма аль дасса школы огонь сними с меня прочь школу дай мне другую одежду хорошую одеть на себя наконец то ты скажешь это ведь правда? Скажи сейчас же отец со мной говорит очень сильно он хочет чтобы все дети до одного года были здесь только взрослые дядя Вернон Эггин\n",
      "----\n",
      "Кетский: так и сказала. Перевод :так рассказала \n",
      "----\n",
      "Кетский: вот ему положили. Перевод : Вот этому положил \n",
      "----\n",
      "Кетский: там биля қусь тийе кәт. Перевод: как человек живет в чуме зимой один целый месяц говорит проверю ее слова говорю проверить меня на верность семье деньги есть у нее семья большая хорошая дети маленькие я одна осталась почему то ведь раньше была женщина это было давно уже очень много лет назад теперь все умерли ну и хорошо что мне тогда пять детей осталось только три сына мальчик старший брат да еще младший ребенок умер недавно летом здесь вот они сейчас гдето далеко\n",
      "----\n",
      "Кетский: қок баат дольда. Перевод: один старик жил в лесу умер летом здесь копать стали очень сильно кричать мальчик рассердился и убежал прочь отсюда вверх по реке большая вода это я помню хорошо сейчас там наверное уже ничего нет ни одного дома ведь давно уж было раньше не то что теперь совсем маленькие были все мальчики да девочки одна бабушка осталась потом умерли ну а мальчиков почемуто нету никого кроме меня самой маленькой была тогда еще дочка есть хорошая девочка тоже умерла ага вот так мне лет\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    inpt = tok.encode(\"Кетский:\", return_tensors=\"pt\")\n",
    "    out = model.generate(inpt.cuda(),\n",
    "                                       num_return_sequences= 2,\n",
    "                                       max_length=100, \n",
    "                                       repetition_penalty=2.0,\n",
    "                                       do_sample=True,\n",
    "                                       top_k=50, top_p=0.95,\n",
    "                                       temperature=0.7)\n",
    "    print(tok.decode([x.item() if x != 50257 else 0 for x in out[0]]).split('<pad>')[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    inpt = tok.encode(\"Людиковский:\", return_tensors=\"pt\")\n",
    "    out = model.generate(inpt.cuda(),\n",
    "                                       num_return_sequences= 2,\n",
    "                                       max_length=100, \n",
    "                                       repetition_penalty=2.0,\n",
    "                                       do_sample=True,\n",
    "                                       top_k=50, top_p=0.95,\n",
    "                                       temperature=0.7)\n",
    "    print(tok.decode([x.item() if x != 50257 else 0 for x in out[0]]).split('<pad>')[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Людиковский: se oli maksimovad da toin entiä kunna hüö. Переводика максимовых а другой дом был \n",
    "----\n",
    "Людиковский: kačo rodiiheze vilgass. Перевод: большая ли была деревня \n",
    "----\n",
    "Людиковский: kuibo oli heil familii. Перевод.: как была у них фамилия \n",
    "----\n",
    "Людиковский: ei minule päivilleh. Перевод: не отстану \n",
    "----\n",
    "Людиковский: märökse kirguttih. Перевод.: всех их звали мярян \n",
    "----\n",
    "Людиковский: miituine külä oli pešan tapettih vasal dakul toin entiedad oldih ned halturimme lapsid ruattail. Перевод: миитунья была жена дякку хальтурила детей няньчила \n",
    "----\n",
    "Людиковский: siid oli daže ičehü lähtedih kolme podvasad oldah. Переводы какие были деревни пелдожское озеро называли \n",
    "----\n",
    "Людиковский: nu ka lähtedih sinn edelleheze do časounah dorok proidiii. Перевод : они как пошли по миру так больше и не видели никого из детей своих женщинью отправили тоже в прислуги к богачам жить там все время были вместе мужалися да рожь жгли на лепписельге около мельницы стояли отдельно от других работников полей тут немного косили или ходили за водой бурлачили вот\n",
    "----\n",
    "Людиковский: häi oli nainu palatezdale prikašče. Перевод: он был женат в прякке на дочери палатозера \n",
    "----\n",
    "Людиковский: nu hutun doškan kodi. Перевод: ну дякку хутун даша \n",
    "----\n",
    "Людиковский: a nüg ei äijät taluoit keda elot. Переводя не знаю кто это был \n",
    "----\n",
    "Людиковский: kaikkid. Перевод : всех \n",
    "----\n",
    "Людиковский: siit tošpäim muurud nokkah. Перевод.: потом мы обратно вернулись \n",
    "----\n",
    "Людиковский: siid oli hänel kaks poigat dakku ainoz burlittud en tiedüt äpit se miks brihaščimme. Перевод: у него было два сына даже три вот эти рыжие были максы да микки \n",
    "----\n",
    "Людиковский: tulemme. Перевод : придем папаша \n",
    "----\n",
    "Людиковский: siid oli kačo rodiiheze vilgass. Перевод: затем была деревнярдиевых \n",
    "----\n",
    "Людиковский: oligo matrouna venalaine vai kunna dorok. Перевод: была ли матрена русская или кто \n",
    "----\n",
    "Людиковский: kaškanaspiäiz enzimöis pidi loitokse sille suole müoti. Перевод: кашканцы прозвали так за сходство с лошадью \n",
    "----\n",
    "Людиковский: nu siit kačo do minä vör müštan. Перевод : я начала понемногу в своей деревне ходить на праздники \n",
    "----\n",
    "Людиковский: siid dälgele oli kačo mari tože. Перевод: затем была маччи после этого был дом мари \n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    inpt = tok.encode(\"Чукотский:\", return_tensors=\"pt\")\n",
    "    out = model.generate(inpt.cuda(),\n",
    "                                       num_return_sequences= 2,\n",
    "                                       max_length=100, \n",
    "                                       repetition_penalty=2.0,\n",
    "                                       do_sample=True,\n",
    "                                       top_k=50, top_p=0.95,\n",
    "                                       temperature=0.7)\n",
    "    print(tok.decode([x.item() if x != 50257 else 0 for x in out[0]]).split('<pad>')[0])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Чукотский: nəsajoqenat. Перевод: сидят \n",
    "----\n",
    "Чукотский: qeeɬ awjetkujŋa. Перевод : ещё немного постою на часах \n",
    "----\n",
    "Чукотский: ətrʔes. Перевод : всё \n",
    "----\n",
    "Чукотский: ətrʔes. Перевод : всё \n",
    "----\n",
    "Чукотский: ənqene ŋinkekin nɬejwewmuri. Перевод: это нас будят и мальчиков тоже спрашивают что вы делали вчера вечером дома до обеда в школе на перемене после уроков во время каникул осенью зимой весной летом перед отъездом из эгвекинота утром следующего дня чтобы не опоздать к поезду домой ранним утречком затем снова весь день проспать под стук колес товарных вагонов идущих с материка потом\n",
    "----\n",
    "Чукотский: mrenti. Перевод мрэнти \n",
    "----\n",
    "Чукотский: nəraea muriŋinet qora ɣatwaamore. Перевод на амгуэмские мы прибыли в сентябре месяце из эгвекинота зимой это было самое холодное время года здесь у нас была хорошая погода а вот летом шёл дождь и тундровые старики рассказывали нам эту историю осенью во второй половине сентября сюда приезжали бригады чтобы помыться перед летней кочёвкой оленей тут же собирали грибы которые потом сушили их тоже доставали\n",
    "----\n",
    "Чукотский: ənkɬamoqenat. Перевод : dmp там народ толпится \n",
    "----\n",
    "Чукотский: ətrʔes. Перевод : конец \n",
    "----\n",
    "Чукотский: ətrʔes. Перевод : всё \n",
    "----\n",
    "Чукотский: ətrʔes. Перевод : всё \n",
    "----\n",
    "Чукотский: qətitʔet ɣatwaamkeeŋe. Перевод : оказывается уронили возле дома \n",
    "----\n",
    "Чукотский: naqam wane ɣəm это mrakwasak. Перевод: а я не умер \n",
    "----\n",
    "Чукотский: ətrʔes. Перевод : всё \n",
    "----\n",
    "Чукотский: ənqenataŋ nɬejwotʔatewiwnin. Перевод : таким образом ловят крюком \n",
    "----\n",
    "Чукотский: telʲopkaqaj Telesimɬir ikwʔi. Перевод : телёпка а ну как же твоё имя на букву тэлефонынгорынкай пей давай глотай личинку я ведь сказала не надо кричать утка умерла вот так и сиди молча варись пока она ещё горячее немного остыла чтобы покрыться паром приготовилась к работе дальше пошла искать тебя по всей сопке прошлась весь день только возле г\n",
    "----\n",
    "Чукотский: ənkɣam kamsameŋe ʔemi. Перевод : и я домой пошла очень рано утром ведь было пологание градусов в десять утра вот так оно это самое утро зимой вообщето говоря ну ладно тогда уж к четырём часам дня начнётся зима а пока что весь день проспала потом на работу вышла там меня уже ждали те сказали чтобы шла чай попить собрались тоже чайник накрыли дверь открыта была они говорят вон сколько народу при\n",
    "----\n",
    "Чукотский: ɣəm trawetʔawjo taŋkojpatqen. Перевод : ktv я пошла домой \n",
    "----\n",
    "Чукотский: telʲopkaqaj Telesimɬerkuri ikwо. Перевод : телёпка ну проснись наконец дай грудь сосать буду говорить тебе кто я такая и что делаю в этой большой белой сумке привезённой из эгвекинота бабушкой моей мамы прямо так сказали тундровые старики вот такие как ты молодые люди приехали на оленьей упряжке с материка привезли сюда нас всех чай попили возле нарт расспросили\n",
    "----\n",
    "Чукотский: ənŋin waɬʔew waj qojwe muri mimiramore. Перевод : у нас такое есть мы его называли мухоморы крюком вытаскивают оттуда личинки овода их там много валяется мёртвых телят вот как они туда попали потом я не знаю это было давнымдавно очень интересно мне рассказывали этот случай наши старики рыбаки тоже приходили просить денег на водку или ещё что им говорили в ответ тогда\n",
    "----\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Карельский: tuomine raidane. Перевод:\" черемуховые ивняки \n",
    "----\n",
    "Карельский: tämö kuin hiiret nazivajeca liew. Перевод: это как его понимать будет \n",
    "----\n",
    "Карельский: kaikki iče. Перевод : все сама \n",
    "----\n",
    "Карельский: konža. Перевод:\" когда \n",
    "----\n",
    "Карельский: niin. Перевод: так \n",
    "----\n",
    "Карельский: oli miula luajittima. Перевод:\" были у меня сделаны \n",
    "----\n",
    "Карельский: ka mie iče ruavoin. Перевод: вот я сам делал \n",
    "----\n",
    "Карельский: niin že i vezilöildä. Перевод; также и у воды тоже бывает что пруд делают для питья или рыбы там где вода хорошая тогда конечно хорошо а если плохая то лучше вообще не надо в такой колодец опускать воду даже просто так нужно знать как правильно делать чтобы самому из него напиться да еще при этом можно было бы зачерпнуть рукой очень уж хорошей колодезной водой попить этой самой чистой родниковой которая хоть немного напоминала ту\n",
    "----\n",
    "Карельский: ka mänimmuozet oldih pelvahašta. Перевод: вот ходили в поле за лен \n",
    "----\n",
    "Карельский: oi. Перевод: ой \n",
    "----\n",
    "Карельский: ka. Перевод: вот \n",
    "----\n",
    "Карельский: ka. Перевод: вот \n",
    "----\n",
    "Карельский: ka min. Перевод: вот что \n",
    "----\n",
    "Карельский: podberiu kerdua tože ennein. Перевод: подберет камень тоже раньше \n",
    "----\n",
    "Карельский: minnat. Перевод: невестки \n",
    "----\n",
    "Карельский: a kuin toko šanotah vielä mid ruavettih. Перевод: а как обычно говорят еще что делали то после успения богородицы в сентябре или октябре этого года крещение во все церкви тогда ходили на причастие исповедуться и Причащаться Святых Христовых Тайн Крещения Господня В тот год когда праздновали Рождество Пресвятой Богородицы<s>\n",
    "А если бы вам сказали например я расскажу про какогонибудь святого который жил\n",
    "----\n",
    "Карельский: ka mändih i lienöw šanow tuhma briha. Перевод: вот пошли и нашли говорит дурной родник в лесу\n",
    "\n",
    "----\n",
    "Карельский: ka mändih i rubiew. Перевод: вот пошли и нашли \n",
    "----\n",
    "Карельский: a nyt. Перевод:\" а сейчас \n",
    "----\n",
    "Карельский: a miäčöt kuin. Перевод: а мяч как \n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
